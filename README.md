# Offline AI Chatbot

A local RAG (Retrieval-Augmented Generation) chatbot using Ollama and Nomic embeddings.

## Project Structure

```
OfflineAiBot/
â”‚
â”œâ”€â”€ app.py                  # Main Flask application
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ .env                    # Environment variables (create from .env.example)
â”œâ”€â”€ .gitignore             # Git ignore rules
â”‚
â”œâ”€â”€ src/                   # Source code
â”‚   â”œâ”€â”€ core/              # Core modules
â”‚   â”‚   â”œâ”€â”€ config.py              # Configuration settings
â”‚   â”‚   â”œâ”€â”€ vector_store_nomic.py  # Vector store with Nomic embeddings
â”‚   â”‚   â””â”€â”€ document_processor.py  # Document processing logic
â”‚   â”‚
â”‚   â””â”€â”€ utils/             # Utility scripts
â”‚       â””â”€â”€ ingest.py              # Document ingestion script
â”‚
â”œâ”€â”€ app/                   # Web application assets
â”‚   â”œâ”€â”€ static/            # CSS, JS, images
â”‚   â””â”€â”€ templates/         # HTML templates
â”‚       â””â”€â”€ index.html
â”‚
â”œâ”€â”€ data/                  # Data directory
â”‚   â”œâ”€â”€ documents/         # Place your documents here (PDF, DOCX, PPTX, TXT)
â”‚   â””â”€â”€ vector_store/      # Generated vector store index
â”‚
â”œâ”€â”€ scripts/               # Setup and utility scripts
â”‚   â”œâ”€â”€ setup.bat          # Windows setup script
â”‚   â””â”€â”€ setup.sh           # Linux/Mac setup script
â”‚
â”œâ”€â”€ models/                # Downloaded models directory
â””â”€â”€ envi/                  # Virtual environment (if using)
```

## Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Set Up Ollama
Make sure Ollama is installed and running:
```bash
ollama pull gemma2:2b
ollama pull nomic-embed-text:v1.5
```

### 3. Add Documents
Place your documents (PDF, DOCX, PPTX, TXT) in the `data/documents/` folder.

### 4. Ingest Documents
```bash
python src/utils/ingest.py
```

### 5. Run the Application
```bash
python app.py
```

Then open your browser to `http://localhost:5000`

## Configuration

Edit `src/core/config.py` to customize:
- LLM model
- Embedding model
- Vector store settings
- Document processing options

## Environment Variables

Create a `.env` file in the root directory:
```
OLLAMA_HOST=http://localhost:11434
LLM_MODEL=gemma2:2b
EMBEDDING_MODEL=nomic-embed-text:v1.5
```

## Features

- ğŸ“„ Support for multiple document formats (PDF, DOCX, PPTX, TXT)
- ğŸ” RAG-based document retrieval
- ğŸ’¬ Interactive chat interface
- ğŸš€ Fully offline operation
- ğŸ¯ Nomic embeddings for better semantic search

## Requirements

- Python 3.8+
- Ollama running locally
- 8GB+ RAM recommended
